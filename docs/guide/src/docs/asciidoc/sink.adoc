:name: Sink Connector
[[_sink]]
= {name}

The {name} consumes records from a Kafka topic and writes the data to Redis.

== Class Name

The sink connector class name is `com.redis.kafka.connect.RedisSinkConnector`.

The corresponding configuration property would be:

[source,properties]
----
connector.class = com.redis.kafka.connect.RedisSinkConnector
----

[[_sink_delivery]]
== At least once delivery
The {name} guarantees that records from the Kafka topic are delivered at least once.

[[_sink_tasks]]
== Tasks

The {name} supports running one or more tasks.
You can specify the number of tasks with the `tasks.max` configuration property.

[[_sink_redis_client]]
include::{includedir}/_redis_client.adoc[leveloffset=+1]

[[_sink_redis_command]]
== Redis Data Structures
The {name} supports the following Redis data-structure types as targets:

[[_sink_collections]]
* Collections: <<_sink_stream,stream>>, <<_sink_list,list>>, <<_sink_set,set>>, <<_sink_zset,sorted set>>, <<_sink_timeseries,time series>>
+
Collection keys are generated using the `redis.key` configuration property which may contain `${topic}` (default) as a placeholder for the originating topic name.
+
For example with `redis.key = ${topic}` and topic `orders` the Redis key is `set:orders`.

* <<_sink_hash,Hash>>, <<_sink_string,string>>, <<_sink_json,JSON>>
+
For other data-structures the key is in the form `<keyspace>:<record_key>` where `keyspace` is generated using the `redis.key` configuration property like above and `record_key` is the sink record key.
+
For example with `redis.key = ${topic}`, topic `orders`, and sink record key `123` the Redis key is `orders:123`.

[[_sink_hash]]
=== Hash
Use the following properties to write Kafka records as Redis hashes:

[source,properties]
----
redis.command   = HSET
key.converter   = <string or bytes> <1>
value.converter = <Avro or JSON> <2>
----
<1> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>
<2> <<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>.
If value is null the key is deleted.

[[_sink_string]]
=== String
Use the following properties to write Kafka records as Redis strings:

[source,properties]
----
redis.command   = SET
key.converter   = <string or bytes> <1>
value.converter = <string or bytes> <2>
----
<1> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>
<2> <<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>.
If value is null the key is deleted.

[[_sink_json]]
=== JSON
Use the following properties to write Kafka records as RedisJSON documents:

[source,properties]
----
redis.command   = JSONSET
key.converter   = <string, bytes, or Avro> <1>
value.converter = <string, bytes, or Avro> <2>
----
<1> <<_sink_key_string,String>>, <<_sink_key_bytes,bytes>>, or <<_sink_value_avro,Avro>>
<2> <<_sink_value_string,String>>, <<_sink_value_bytes,bytes>>, or <<_sink_value_avro,Avro>>.
If value is null, the entire key will be deleted.

[source,properties]
----
redis.command         = JSONMERGE
key.converter         = <string, bytes, or Avro> <1>
value.converter       = <string, bytes, or Avro> <2>
redis.json.path       = <Header Attribute> <3>
redis.json.path.fixed = <Fixed JSON Path> <4>
----
<1> <<_sink_key_string,String>>, <<_sink_key_bytes,bytes>>, or <<_sink_value_avro,Avro>>
<2> <<_sink_value_string,String>>, <<_sink_value_bytes,bytes>>, or <<_sink_value_avro,Avro>>.
<3> <<_sink_header_json_path,Header JSON Path>>
<4> <<_sink_fixed_json_path,Fixed JSON Path>>
If value is null, the JSON object at the specified path will be deleted. If no path is set, the entire key will be deleted.
+
The `JSONMERGE` command has the same functionality as `JSONSET` but with the added ability to specify a JSON path.

[[_sink_stream]]
=== Stream
Use the following properties to store Kafka records as Redis stream messages:

[source,properties]
----
redis.command   = XADD
redis.key       = <stream key> <1>
value.converter = <Avro or JSON> <2>
----
<1> <<_sink_collections,Stream key>>
<2> <<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>

[[_sink_list]]
=== List
Use the following properties to add Kafka record keys to a Redis list:

[source,properties]
----
redis.command = <LPUSH or RPUSH> <1>
redis.key     = <key name> <2>
key.converter = <string or bytes> <3>
----

<1> `LPUSH` or `RPUSH`
<2> <<_sink_collections,List key>>
<3> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to push to the list

The Kafka record value can be any format.
If a value is null then the member is removed from the list (instead of pushed to the list).

[[_sink_set]]
=== Set
Use the following properties to add Kafka record keys to a Redis set:

[source,properties]
----
redis.command = SADD
redis.key     = <key name> <1>
key.converter = <string or bytes> <2>
----
<1> <<_sink_collections,Set key>>
<2> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value can be any format.
If a value is null then the member is removed from the set (instead of added to the set).

[[_sink_zset]]
=== Sorted Set
Use the following properties to add Kafka record keys to a Redis sorted set:

[source,properties]
----
redis.command = ZADD
redis.key     = <key name> <1>
key.converter = <string or bytes> <2>
----
<1> <<_sink_collections,Sorted set key>>
<2> <<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>: Kafka record keys to add to the set

The Kafka record value should be `float64` and is used for the score.
If the score is null then the member is removed from the sorted set (instead of added to the sorted set).

[[_sink_timeseries]]
=== Time Series
Use the following properties to write Kafka records as RedisTimeSeries samples:

[source,properties]
----
redis.command = TSADD
redis.key     = <key name> <1>
----
<1> <<_sink_collections,Timeseries key>>

The Kafka record key must be an integer (e.g. `int64`) as it is used for the sample time in milliseconds.

The Kafka record value must be a number (e.g. `float64`) as it is used as the sample value.


[[_sink_data_formats]]
== Data Formats

The {name} supports different data formats for record keys and values depending on the target Redis data structure.

[[_sink_key]]
=== Kafka Record Key
The {name} expects Kafka record keys in a specific format depending on the configured target <<_sink_redis_command,Redis data structure>>:

[options="header",cols="h,1,1"]
|====
|Target|Record Key|Assigned To
|Stream|Any|None
|Hash|String|Key
|String|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Key
|List|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|Set|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|Sorted Set|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Member
|JSON|<<_sink_key_string,String>> or <<_sink_key_bytes,bytes>>|Key
|TimeSeries|Integer|Sample time in milliseconds
|====

[[_sink_key_string]]
==== StringConverter
If record keys are already serialized as strings use the StringConverter:

[source,properties]
----
key.converter = org.apache.kafka.connect.storage.StringConverter
----

[[_sink_key_bytes]]
==== ByteArrayConverter
Use the byte array converter to use the binary serialized form of the Kafka record keys:

[source,properties]
----
key.converter = org.apache.kafka.connect.converters.ByteArrayConverter
----

[[_sink_value]]
=== Kafka Record Value
Multiple data formats are supported for Kafka record values depending on the configured target <<_sink_redis_command,Redis data structure>>.
Each data structure expects a specific format.
If your data in Kafka is not in the format expected for a given data structure, consider using {link_smt} to convert to a byte array, string, Struct, or map before it is written to Redis.

[options="header",cols="h,1,1"]
|====
|Target|Record Value|Assigned To
|Stream|<<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>|Message body
|Hash|<<_sink_value_avro,Avro>> or <<_sink_value_json,JSON>>|Fields
|String|<<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>|Value
|List|Any|Removal if null
|Set|Any|Removal if null
|Sorted Set|Number|Score or removal if null
|JSON|<<_sink_value_string,String>> or <<_sink_value_bytes,bytes>>|Value
|TimeSeries|Number|Sample value
|====

[[_sink_value_string]]
==== StringConverter
If record values are already serialized as strings, use the StringConverter to store values in Redis as strings:

[source,properties]
----
value.converter = org.apache.kafka.connect.storage.StringConverter
----

[[_sink_value_bytes]]
==== ByteArrayConverter
Use the byte array converter to store the binary serialized form (for example, JSON, Avro, Strings, etc.) of the Kafka record values in Redis as byte arrays:

[source,properties]
----
value.converter = org.apache.kafka.connect.converters.ByteArrayConverter
----

[[_sink_value_avro]]
==== Avro
[source,properties]
----
value.converter                     = io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url = http://localhost:8081
----

[[_sink_value_json]]
==== JSON
[source,properties]
----
value.converter                = org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable = <true|false> <1>
----
<1> Set to `true` if the JSON record structure has an attached schema

[[_sink_header_json_path]]
==== Header JSON Path
The JSON attribute in the header of the Kafka message based on which the JSON path will be dynamically set.

[source,properties]
----
redis.json.path = <Header Attribute>
----

[[_sink_fixed_json_path]]
==== Fixed JSON Path
The fixed JSON path to set within the value in the header of the Kafka message. This path will be used if the dynamic path is not present.

[source,properties]
----
redis.json.path.fixed = <Fixed JSON Path>
----

[[_sink_config]]
== Configuration

[source,properties]
----
connector.class       = com.redis.kafka.connect.RedisSinkConnector
topics                = <Kafka topic> <1>
redis.uri             = <Redis URI> <2>
redis.command         = <HSET|SET|JSONSET|JSONMERGE|XADD|RPUSH|SADD|ZADD|TSADD> <3>
redis.json.path       = <Header Attribute> <4>
redis.json.path.fixed = <Fixed JSON Path> <5>
key.converter         = <Key converter> <6>
value.converter       = <Value converter> <7>
----
<1> Kafka topics to read messages from.
<2> <<_sink_redis_client,Redis URI>>.
<3> <<_sink_redis_command,Redis command>>.
<4> <<_sink_header_json_path,Header JSON Path>>.
<5> <<_sink_fixed_json_path,Fixed JSON Path>>.
<6> <<_sink_key,Key converter>>.
<7> <<_sink_value,Value converter>>.
